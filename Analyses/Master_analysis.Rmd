---
title: 'ML Final Project: Analysis'
author: "Joe Marlo, Andrew Pagtakhan, George Perrett, Bilal Waheed"
date: "January 27, 2020"
output: pdf_document
---

# INTRO AND DATA DESCRIPTION

## Research Question: Are there underlying patterns of groupings between NBA team compensation vs. overall team skillsets?

## Full project link
https://github.com/joemarlo/ML-NBA

The project is an unsupervised approach to discover underlying patterns or groupings between NBA compensation vs. overall team skillsets. It uses K-means, hierarchical, and model-based clustering along with other techniques and tools such as principal component analysis, standardizing, scaling, and web-scraping.

The data includes NBA statistics for over 3,000 players, 60+ seasons, and over 50 features per players
Measurement Scales: Numerical totals and percentages for features such as: points, assists, rebounds, player attributes (height, weight, college attended, etc.)
Time period: 1950 - 2017 (through 2016 - 2017 season). We are selecting data for the 2016 - 2017 season.

Key Feature Set: 
MP_pg: Minutes played per game
FG_pg: Field goals made per game
FGA_pg: Field goal attempts per game
3P_pg: 3-point shots made per game
3PA_pg 3-points shot attempts per game
2P_pg: 2-point shots made per game
2PA_pg: 2-point shots attempted per game
FT_pg: Free throw shots made per game
FTA_pg: Free throw shots attempted per game
TRB_pg: Total rebounds (offensive + defensive) per game
AST_pg: Total assists per game
STL_pg: Total steals per game
BLK_pg: Total blocks per game
PTS_pg: Total points made per game


Additional Features:
win_pct: team winning percentage during the regular seasong (Total wins / total games played) 
Player: First and last name of NBA player
Team: NBA team player played on. For multiple teams, this is the team the player played on for the most minutes 
Position: NBA position, e.g. C = Center, PF = Power Forward, SG = Shooting Guard,  PG = Point Guard
Salary: Player salary (USD)
RPM: Real Plus/Minus. Player's average impact in terms of net point differential per 100 offensive and defensive possessions
VORP: Value Over Replacement Player. Measure to estimate each player’s overall contribution to the team
PER: Player Efficiency Rating. Measures player's overall contributions across different statistics.


Source: https://www.kaggle.com/drgilermo/nba-players-stats#player_data.cs

# Analysis Methods
For this analysis, we applied the following modeling techniques to analysze NBA player data:
* Principal Component Analysis
* Hierarchical Clustering
* K-Means
* Model-Based clustering

# DATA EXPLORATION AND TRANSFORMATION

The data cleaning was done in separate R scripts. 
https://github.com/joemarlo/ML-NBA
Steps:
* Filtered data to the 2016 - 2017 season
* Assigned player to one team based on the most minutes he played for, including stats across all teams played for
* Scraped player salaries and RPM data from ESPN website, using fuzzy matching on player names to join to the main dataset
* Scaled/Transformed data using cube root and standardizing. For model-based clustering, various transformations such as Log + 1, square root, cube root, and Box-Cox were applied. The cube root yielded the best transformation to normalize the data. This was validated using QQ plots. We scaled the data for Hierarchical and K-Means to put all features on an equally weighted basis. For example, minutes played per game vs. blocks per game are on different ranges, so scaling these puts them on a comparable basis.

## Group Member Roles
We all collaboratively contributed to the project in all aspects. Specifically, Joe contributed to scraping the player salary, additional player features and visualizations. Andrew contributed to cleaning the data, hierarchical/K-means models, and post-cluster analysis. Bilal contributed to the hierarchical/K-Means modeling and the write-up. George contributed to the model-based clustering and data transformation.

# Analysis

## Exploratory Data Analysis

```{r setup, include=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
library(dendextend)
library(factoextra)
library(GGally)
library(ggfortify)
library(ggrepel)
library(gridExtra)
library(knitr)
library(mclust)
library(NbClust)
library(plyr)
library(rgl)
library(tidyverse)

# set for reproducible results
set.seed(14)

# set theme for ggplot
theme_set(theme_minimal())

# clear variables
rm(list = ls())

# set working directory
opts_knit$set(root.dir = normalizePath('..'))

# define file name for analysis
filename <- 'Data/season_stats_clean.csv'
```

We decided to use the 14 features in our analysis (on a per game basis) because they provide a good balance of offensive (e.g Pts, Ast) and defensive stats (e.g. Reb, Blk). This is more likely to provide more balanced groupings between those who are more offensive and those who are better at defense.

With these features, we will explore the distribution of each future.

```{r}
# load data
nba <- read.csv(filename)
# replace NA values in RPM column with 0s
nba$RPM[is.na(nba$RPM)] <- 0
```

```{r echo=FALSE}

# plot distributions of key per game stats
features_pg <- grep('_pg', names(nba), value = TRUE)
nba_feat <- nba[ , features_pg]

nba_feat %>% 
  pivot_longer(cols = everything()) %>%
  ggplot(aes(x = value)) +
  geom_density() +
  facet_wrap(~name, scales = "free") + 
  labs(title = 'Feature Densities (Untransformed)', 
       x = '')
```

We calculate the variance to look at the spread of each feature.

```{r}
# calculate variance
var_table <- round(apply(nba_feat, MARGIN = 2, FUN = var), 2)
var_df <- data.frame(var_table)
colnames(var_df) <- 'Variance'
kable(var_df, caption = 'Feature Variance')
```


```{r, echo=FALSE, warning=FALSE}
# look at pairs plots for key stats
feat_plot <- c('MP_pg', 'PTS_pg', 'AST_pg', 'TRB_pg', 
               'STL_pg', 'BLK_pg')
nba_feat_plot_pos <- nba[ , feat_plot]
ggpairs(nba_feat_plot_pos,
        columns = 1:(length(names(nba_feat_plot_pos)) - 1),
        progress = FALSE, 
        mapping = ggplot2::aes(colour = nba$Pos, alpha = 0.4),
        upper = list(continuous = wrap('cor', size = 0))
)
```


We then scaled the data because the ranges of the data can be different. A good example is minutes and blocks per game - most players will have more minutes per game than blocks per game.

## Run Principal Component Analysis

Before running any clustering algorithms, we will perform Principal Component Analysis to determine if there are any inherent groupings among players.

```{r}
nba_feat_sc <- scale(nba_feat)

# run PCA
nba_pca <- prcomp(nba_feat_sc)
summary(nba_pca)
```

```{r}
# create plot PCA data function
plot_pca <- function(object, frame = FALSE, x = 1, y = 2, 
                     data, colour, title, label, leg_title) {
  # plots data in PCA space 
  # object = PCA or K-Means object
  # x = which PC for x-axis (1, 2, ,3, etc..)
  # y = which PC for y-axis (1, 2, 3, etc..)
  # object: PCA or K-means object
  # data = underlying data
  p <- autoplot(nba_pca, x = x, y = y, data = nba, colour = colour, frame = frame) + 
        ggtitle(title) + 
        # center title
        theme(plot.title = element_text(hjust = 0.5)) + 
        geom_label_repel(aes(label = label),
                        box.padding   = 0.35, 
                        point.padding = 0.5,
                        segment.color = 'grey50') + 
    ## This is supposed to override the autoplot legend title.
    ## Only works when plotting PCA directly. Does not work for HCL and KM objects
    labs(colour = leg_title)
        # theme_classic()
  return(p)
}
```


The first 2 principal components explain the majority of the variance in the feature set, we will plot the data in these two dimensions to better assess player similarities. 

At first glance, there are differences between players based on overall statistics. For example, LeBron James and James Harden are near each other, indicating star players may be grouped together. There are also similarities based on player position. For example, Centers/Power Forwards such as Anthony Davis and Karl-Anthony Towns are in the bottom of the chart.

```{r}
# Labels: Players who played more than 36 min per game or less than 3 min per game
labels_pca <- ifelse(nba$MP_pg >= 36 | nba$MP_pg <= 3, 
                    as.character(nba$Player), '')
title_pca <- paste0('PCA: NBA - ', ncol(nba_feat) ,' features')

# Plot first two components with positions
plot_pca(nba_pca, data = nba, colour = 'Pos', 
         label = labels_pca, title = title_pca, leg_title = 'Position'
          )
```


```{r, include = FALSE}
# plot in 3d
pca_plot <- nba_pca$x[ , 1:3]
plot3d(pca_plot)
```


## Clustering

### Hierarchical clustering
The first method of clustering we will try is hierarchical clustering. The dendrogram can help provide a visual aid in the number of clusters we can start to use.
```{r}
# distance matrix for features
nba_dist_sc <- dist(nba_feat_sc, method = 'euclidean')
  
# try single, centroid, and ward (D2) linkage hier clustering
hcl_single <- hclust(d = nba_dist_sc, method = 'single')
hcl_centroid <- hclust(d = nba_dist_sc, method = 'centroid')
hcl_ward <-  hclust(d = nba_dist_sc, method = 'ward.D2')
```


### ADJUST WIDTH for PLOTS
```{r, fig.align = "center", fig.width=6, fig.height=7}
par(mfrow = c(3, 1))
# nearest neighbors method
plot(hcl_single, hang = -1, main = 'Single Linkage', 
     labels = FALSE, xlab = '', sub = '')
# groups centroid
plot(hcl_centroid, hang = -1, main = 'Centroid Linkage', 
     labels = FALSE, xlab = '',  sub = '')
# Ward’s minimum variance method, 
# with dissimilarities are squared before clustering
dend <- as.dendrogram(hcl_ward)
hcl_k <- 4
dend_col <- color_branches(dend, k = hcl_k)
plot(dend_col, main = paste0('Ward (D2) Linkage: K = ', hcl_k), 
     labels = FALSE)
```


Since the Ward dendrogram seems to be the best among the three, we will look at its distribution for 3 and 4 clusters. We chose these initial groupings because this provides a good start to separate and distinguish between player groups.

```{r}
# add cluster labels to main data
nba$hcl_ward_labs_three <- cutree(hcl_ward, k = 3)
nba$hcl_ward_labs_four <- cutree(hcl_ward, k = 4)

hcl_df_three <- data.frame(table(nba$hcl_ward_labs_three))
hcl_df_four <- data.frame(table(nba$hcl_ward_labs_four))
col_names <- c('Clusters', 'Count')
colnames(hcl_df_three) <- col_names
colnames(hcl_df_four) <- col_names


print(kable(hcl_df_three, caption = 'Three Clusters'))
print(kable(hcl_df_four, caption = 'Four Clusters'))

```


Visualizing the clusters in PC space, we see clear separation in the 4-cluster solution vs the 3-cluster method based on apparent skillsets. For example, LeBron James and James Harden (star players) are in one cluster, whereas John Lucas and Danuel House (lower performers) are in the left-most cluster, opposite to the 'star player' cluster on the right. These will be explored further when we look at player statistics by cluster.

```{r}
# add labels to data
nba$hcl_ward_three <- factor(cutree(hcl_ward, k = 3))
nba$hcl_ward_four <- factor(cutree(hcl_ward, k = 4))

# player names to include in plot
hcl_labels <- ifelse(nba$MP_pg >= 36 | nba$MP_pg <= 2.5 |
                     (nba$MP_pg >= 28.8 & nba$MP_pg <= 29) | 
                     (nba$MP_pg >= 25 & nba$MP_pg <= 25.2),
                     as.character(nba$Player), '' )

# elements to loop over
hcl_labs <- names(nba %>% select(tail(names(.), 2)))
hcl_ks <- c(3, 4)

# plot hclust labels superimposed over PCA
for (i in seq_along(hcl_labs)) {
  p <- plot_pca(nba_pca, frame = TRUE, 
                data = nba, colour = hcl_labs[i],
                title = paste0('PCA: ', hcl_ks[i], ' clusters (Hclust)'),
                label = hcl_labels,
                leg_title = 'Clusters'
  )

  print(p)
}
```


```{r, include = FALSE}
# plot 3d
hcl_pca <- cbind(nba_pca$x[ , 1:3], enframe(nba$hcl_ward_labs_four))
pca_plot <- hcl_pca
plot3d(pca_plot, 
       col = hcl_pca$value, size = 15)
```



### Optimize number of clusters

To optimize the number of clusters, we used two index methods: Calinski-Harabasz and Silhouette. These methods measure how well separated clusters are, and how homogenous data is within each cluster. Both indices yield 2 as the optimal number.

Although the indices say 2 is optimal, these does not provide enough distinction or separation among players. 
However, the Silhouette index provides 4 as a local maximum. This provides a more practical distinction among players.

Methods: Calinski-Harabasz index and Silhouette
```{r}

# get optimal cluster sizes 
cluster_sizes_hcl <- NbClust(data = nba_feat_sc,
                          # it will likely be harder to interpret clusters
                          # past this amount
                          max.nc = 6,
                          method = 'ward.D2',
                          index = 'ch')

# plot C(G)
plot(names(cluster_sizes_hcl$All.index),
     cluster_sizes_hcl$All.index,
     main = 'Calinski-Harabasz index: HCL',
     type = 'l')
```

```{r}
# get optimal cluster sizes 
cluster_sizes_hcl <- NbClust(data = nba_feat_sc,
                          # it will likely be harder to interpret clusters
                          # past this amount
                          max.nc = 6,
                          method = 'ward.D2',
                          index = 'silhouette')

# plot C(G)
plot(names(cluster_sizes_hcl$All.index),
     cluster_sizes_hcl$All.index,
     main = 'Silhouette index: HCL',
     type = 'l')
```


## K-Means

### Optimize number of clusters

Similar to the optimization for hierarhical, we found similar results for K-Means. We decided with the 4-cluster solution based on the Silhouette index (local maximum.)

Method: Calinski-Harabasz index
```{r}
# get optimal cluster sizes 
cluster_sizes_km <- NbClust(data = nba_feat_sc,
                          # it will likely be harder to interpret clusters
                          # past this amount
                          max.nc = 6,
                          method = 'kmeans',
                          index = 'ch')

# plot C(G)
plot(names(cluster_sizes_km$All.index),
     cluster_sizes_km$All.index,
     main = 'Calinski-Harabasz index: K-Means',
     type = 'l')

```


```{r}
# get optimal cluster sizes 
cluster_sizes_km <- NbClust(data = nba_feat_sc,
                          # it will likely be harder to interpret clusters
                          # past this amount
                          max.nc = 6,
                          method = 'kmeans',
                          index = 'silhouette')

# plot C(G)
plot(names(cluster_sizes_km$All.index),
     cluster_sizes_km$All.index,
     main = 'Silhouette index: K-Means',
     type = 'l')

```


### K-means clustering with 4 groups
```{r}
km_k <- 4
km_four <- kmeans(x = nba_feat_sc,
            centers = km_k,
            nstart = 100,
            algorithm = 'Hartigan-Wong')

nba$km_labs_four <- factor(km_four$cluster)
```



```{r echo=FALSE}
# swap labels 3 and 4 so that label 4 are the best players
nba$km_labs_four <- factor(ifelse(nba$km_labs_four == 4, 3, 
                          ifelse(nba$km_labs_four == 3, 4, 
                          nba$km_labs_four)))

```

Compared to the hierarchical solution presented earlier, there is cleaner separation in the K-Means plot. We will see how these clusters are separated by inspecting features within each group.

```{r}
# plot k-means clusters in PC space
# Labels: Players who played more than 36 min per game or less than 3 min per game
km_labels <- ifelse(nba$MP_pg >= 36 | nba$MP_pg <= 3, 
                         as.character(nba$Player), '' )

plot_pca(km_five, data = nba, frame = TRUE, colour = 'km_labs_four',
          title = paste0('PCA: ', km_k, ' clusters (K-means)'),
          label = km_labels, leg_title = 'Clusters')
```

One explanation for the imbalanced number of players across clusters is that player skillset level is inherently imbalanced. This imbalance is reflected in the univariate plots, where most densities were positively skewed. This suggests that there a few players whose statistics significantly exceed those of the average player. This is also reflected in the average statistics by cluster shown below.

```{r echo=FALSE}
# get distribution of players in each cluster
km_labs <- data.frame(table(nba$km_labs_four))
colnames(km_labs) <- col_names

kable(km_labs, caption = 'K-Means Player Distribution')

```



```{r}
# averages by cluster
nba_km_avg <- data.frame(nba
                            %>% select(km_labs_four, MP_pg, PTS_pg, TRB_pg,
                                       AST_pg, BLK_pg, STL_pg, VORP, PER, RPM)
                            %>% group_by(km_labs_four)
                            %>% summarise_all(list(mean))
                            )

kable(nba_km_avg, caption = 'Average Stats by Cluster')
```


### Model-Based Clustering

To test alternative approaches to Hierarchical and K-Means clustering, we will perform model-based clustering.
Prior to modeling, we will transform the data using the cube root. This normalizes the data, which is a key assumption in model-based clustering. We tested other transformations such as the square root, Box-Cox, and log + 1. Of these, The cube root yielded the best transformation for model-based clustering.

```{r}
# transform features
nba_feat_cr <- (nba_feat) ^ (1/3)
# scale transformed features
nba_feat_cr_sc <- scale(nba_feat_cr)

```

```{r echo=FALSE}
# plot transformed variables
nba_feat_cr_sc %>% as_tibble() %>%
  pivot_longer(cols = everything()) %>%
  ggplot(aes(x = value)) +
  geom_density() +
  facet_wrap(~name, scales = "free") + 
  labs(title = 'Feature Densities (Transformed)', 
       x = '')

```

The model-based clustering produced a five-cluster solution based on the BIC. Compared to the previous methods, model-based clustering tended to group players by moreseo by style of play. On the other hand, K-Means and Hierarchical appear to cluster on overall skillsets across statistics. The clusters also align more with the original PCA plot by position. This type of clustering suggests emphasis on style of play, e.g. better rebounders/blockers, better passers, scorers, etc.

```{r}
# run model
player_clust.mcl <- Mclust(nba_feat_cr_sc)
summary(player_clust.mcl)
plot(player_clust.mcl, what = "BIC")

```


```{r}
# plot results
fviz_mclust(player_clust.mcl, "classification", geom = "point")
```

```{r}
# add cluster labels to plot
nba$mcl_labs <- player_clust.mcl$classification

```


### Compare methods between Clusters

Between HCL and KM, the maximum possible agreement between clusters is 87% (424 / 486).
Between HCL and MCL, the maximum possible agreement between clusters is 30% (148 / 486).
Between KM and MCL, the maximum possible agreement between clusters is 41% (201 / 486).

Based on the analysis, MCL tends to group players by position, whereas HCL and KM tend to cluster based on overall player statistics. This conclusion was reached based on inspecting distributions of player positions across the clustering methods. Because we are looking at player statistics and team compensation, the model-based clustering is not an ideal fit for this purpose.


```{r}
# run crosstabs between cluster methods
xtab_hcl_km <- xtabs(~nba$hcl_ward_four + nba$km_labs_four)
xtab_hcl_mcl <- xtabs(~nba$hcl_ward_labs_four + nba$mcl_labs)
xtab_km_mcl <- xtabs(~nba$km_labs_four + nba$mcl_labs)

xtab_hcl_km
xtab_hcl_mcl
xtab_km_mcl
```

## Final Cluster selection
K-Means (4 clusters) was the optimal solution. Comparing the HCL and KM cluster plots (per above) reveals the K-Means produces clearer separation of players based on overall skillsets. 

We validated this by comparing the clusters against advanced statistics. PER, VORP, and RPM are advanced statistics commonly used to assess general player performance. None of these statistics were used in the cluster modeling.
```{r echo=FALSE}
# plot averages by cluster
nba_km_avg %>% 
  select(km_labs_four, VORP, PER, RPM) %>% 
  pivot_longer(cols = c("VORP", "PER", "RPM")) %>%
  ggplot(aes(x = km_labs_four, y = value)) +
  geom_col() +
  facet_wrap(~name, scales = 'free') + 
  labs(title = 'Overall Player Performance by Cluster',
       subtitle = 'Average Advanced Statistics',
       x = 'Cluster Group', 
       y = '')

```


## Post-Cluster Analysis

We will now look at different statistics and demographics to assess cluster membership and draw insights.

```{r}
# Map cluster labels to KM values
nba$km_labs_opt_names <- mapvalues(nba$km_labs_four,
                             from = c(1, 2, 3, 4),
                             to = c('Subpar', 'Bench', 'Role', 'Star'))
```

There is potential to update salaries based on player tiers. For example, Chandler Parsons was paid 22M but is considered a low performer, and is paid more than star players such as Steph Curry (12M) and Kawhi Leonard (17.6M). This shows that salary is not necessarily strongly correlated with player performance.

### Clusters vs. Player Salaries
```{r echo=FALSE}
# salary vs. advanced stats, overlayed with clusters

nba$km_labs_opt <- nba$km_labs_four

# cluster label to use
cl_label <- "km_labs_opt_names"

# plot PER vs. salary
ggplot(data = nba, aes(x = Salary, y = PER)) + 
  geom_point(aes_string(color = cl_label)) + 
  geom_label_repel(aes(label = labels_pca),
                box.padding   = 0.35, 
                point.padding = 0.5,
                segment.color = 'grey50') + 
  ggtitle('PER vs. Salary') + 
  scale_x_continuous(labels = scales::dollar)
  # theme_classic()
```


```{r}
# Highest Paid players in Lowest Tier
kable(head(data.frame(nba
           %>% select(Player, G, MP_pg, Tm, 
                      Salary, PER, cl_label)
           %>% filter(km_labs_opt_names == 'Subpar')
           %>% arrange(desc(Salary))
           )), 
      caption = 'Highest Paid Players: Subpar')

```

```{r}
# Lowest Paid players in highest tier
kable(head(data.frame(nba
           %>% select(Player, G, MP_pg, Tm, Salary, PER, cl_label)
           %>% filter(km_labs_opt_names == 'Star')
           %>% arrange(Salary)
           )), 
      caption = 'Lowest Paid Players: Star Players')

```



The chart below indicates how much a player is overpaid or underpaid with respect to their cluster average salary. For example, Chandler Parsons was paid 22M vs. the cluster salary average (~2M), indicating he was overpaid. It is important to note that there are players who may have been injured, so their statistics may not be commensurate to their salaries.

```{r echo=FALSE}
# create list of players to highlight on the player salary diff plot

players.to.show <- c("Carmelo Anthony", "Chandler Parsons", "Trey Lyles", "James Johnson", "Devin Booker")

# create data frame that includes the differnece in player salary and cluster avg salary
salary.diff.df <- nba %>%
  group_by(km_labs_four) %>%
  mutate(Clust_Salary = mean(Salary)) %>%
  ungroup() %>%
  mutate(Salary_diff = (Clust_Salary - Salary) / 1000000,
         Player = as.character(Player),
         Show = Player %in% players.to.show,
         Label = if_else(Show, paste0(Player, ": ", km_labs_opt_names), ""),
         Color = if_else(Show, "grey80", NULL)) %>% 
  arrange(Salary_diff)

salary.diff.df %>% 
  ggplot(aes(y = reorder(Player, Salary_diff), x = Salary_diff,
             color = Salary_diff, labels = Label)) +
  geom_label_repel(aes(label = Label)) +
  geom_point() +
  geom_vline(xintercept = 0,
             linetype = "dashed",
             color = "grey70") +
  scale_y_discrete(breaks = NULL) +
  scale_x_continuous(labels = scales::dollar) +
  scale_color_gradient2(mid = "grey80", high = "green4") +
  coord_cartesian(ylim = c(-20, 500)) +
  labs(title = "Finding Over/underpaid players based on cluster membership",
       subtitle = "Difference between player salary and respective cluster mean salary",
       y = "",
       x = "Difference between salary and cluster mean (in millions)\n Negative indicates overpaid") +
  theme_minimal() +
  theme(legend.position = "none",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

```


### Team Compensation and Performance vs clusters


```{r}
# convert labels to numeric
nba$cl_lab_numeric <- as.numeric(nba[ , cl_label])
nba_team <- data.frame(nba 
                       %>% select(Tm, Salary, win_pct, cl_lab_numeric) 
                       %>% group_by(Tm) 
                       %>% summarise(team_salary = sum(Salary),
                                     win_pct = mean(win_pct),
                                     avg_clust = mean(cl_lab_numeric))
                       )

# order by descnding average cluster label
arrange(nba_team, desc(avg_clust))
```

Although a team can have better players on average clusters, there are many variables at play here. A team can be better on average but poor management or coaching can affect a team's overall performance, e.g. NYK. Interestingly, GSW did not have the highest average cluster rating, because their bench is not very strong. This speaks to the strong influence that starter players can have on team performance. Another interesting note is that teams can play well even if they do not have many all-stars or a strong overall team, e.g BOS. This could be driven by great coaching and team chemistry. It is important to note that items such as injuries could greatly influence win %, even if players have high ratings. 

Although there is a correlation between overall team salary and win %, it is interesting that average player rating does not necessarily align with overall win %.

```{r echo=FALSE}
# plot
ggplot(nba_team, 
       aes(x = team_salary, y = win_pct, color = avg_clust)) + 
  geom_smooth(method = 'lm', formula = y ~ x, se = FALSE, color = 'gray') +
  geom_point() + 
  scale_x_continuous(labels = scales::dollar_format()) +
  geom_label_repel(label = nba_team$Tm) + 
  scale_color_gradient2(low = 'red', 
                        midpoint = mean(nba_team$avg_clust),
                        mid = 'gray',
                        high = 'green4') + 
  labs(title = 'Win % vs. Team Salary')  
  # theme_classic()
```


```{r}
# Inspect some teams
teams_sample <- c('NYK', 'GSW', 'BOS')

teams_sample_list <- list(rep(NA, length = length(teams_sample)))
for (i in seq_along(teams_sample)) {
  teams_sample_list[[i]] <- nba %>% 
    filter(Tm == teams_sample[i]) %>%
    arrange(desc(km_labs_four)) %>%
    select(Player, PER, cl_label)
}

# change index to see different teams
team_index <- 2
team_sample <- data.frame(teams_sample_list[team_index])
colnames(team_sample) <- c('Player', 'PER', 'Cluster')
kable(team_sample, caption = paste0(teams_sample[team_index], ': Players'))
```


